## **Reflective Activity 1: Ethics in Computing in the Age of Generative AI**



The rise of generative AI since late 2022 has significantly disrupted the computing field. From text and image generation to code completion and design automation, its impact is being felt across industries. While the core ideas behind artificial intelligence are not new, the recent pace and scale of adoption have raised fresh ethical and professional questions. Reflecting on Correa et al. (2023) and Deckard (2023), I found that the primary concern is not just what AI can do, but how computing professionals should respond to it.

Correa et al. (2023) explore how different countries and organisations are developing their frameworks to govern AI. A key challenge they highlight is the lack of global agreement. While many guidelines refer to values such as fairness, accountability, and transparency, the way these values are applied varies widely. For instance, the European Union prioritises individual rights and risk classification, while China focuses more on national stability and collective benefit. In the United States, there is a more substantial reliance on industry self-regulation rather than centralised government policy. 

This variation creates uncertainty around what ethical standards professionals should follow, especially in international or cross-border projects. Deckard (2023) suggests that ethics should not be treated as a final step or an external audit. Instead, ethical considerations should be built into the design process from the start. This view echoes earlier efforts to embed ethics into computing practice. One example is the Menlo Report, which highlighted the need for proactive ethical governance in ICT research (Finn and Shilton, 2023). This means that computing professionals need to understand the potential social and legal impacts of their work as deeply as they understand the technical aspects. 

An example of these ethical tensions is seen in the use of generative AI tools in education and software development. Tools such as ChatGPT and GitHub Copilot are becoming common in both academic and workplace settings. On the one hand, they can improve efficiency and support learning. On the other hand, there are concerns around originality, authorship, data privacy, and code quality. Studies like Zhou et al. (2022) show that Copilot has generated insecure or duplicated code snippets taken from public repositories, which could raise legal and security concerns.

This places computing professionals in a difficult position. Relying too heavily on such tools without properly reviewing the outputs may lead to mistakes or copyright breaches. Furthermore, if the origins of the AI-generated content are not transparent, it becomes difficult to assign responsibility when things go wrong. This uncertainty is not only a technical issue but also a professional and legal one.

To address these challenges, I believe there should be a stronger emphasis on practical, actionable ethics in computing education and industry practice. The following steps seem particularly relevant:

* **Incorporate ethics into technical training**: Ethics should be included in programming and machine learning modules, rather than taught separately. This helps students and professionals develop the habit of ethical reflection throughout the design and development process.  
* **Label AI-generated content clearly**: Whether in creative projects, academic work, or software products, clearly disclose when AI was used. This would support transparency and allow users to make informed decisions.  
* **Establish independent ethical review for high-risk systems**: Projects that are likely to affect public safety, rights, or trust should undergo review by diverse panels that include domain experts, ethicists, and community representatives.

These actions would not only help manage legal and professional risks but also improve public trust in AI systems. Correa et al. (2023) note that although many principles are shared globally, implementation tends to differ. Similarly, Fjeld et al. (2020) found that many AI ethics frameworks agree on core principles, such as transparency, fairness, and accountability. However, how these principles are enforced or interpreted still differs across contexts. Creating consistent practices within organisations, institutions, or regions may be more achievable in the short term than pushing for international standardisation.

This reflection also made me consider the expectations placed on students and early-career developers. Generative AI tools are now built into many of the platforms we use for learning and productivity. Rather than avoiding them, I believe it is essential to use them responsibly, with awareness of their limitations. For instance, reviewing the quality and origin of AI-generated output is essential before using it in any serious context. Ethical computing involves good judgment as much as technical skill.

As I reflect on the role of generative AI in education, I also recognise the irony of relying on AI tools, including in the preparation of academic work. This underscores the very challenges discussed: balancing the efficiency AI offers with the responsibility to engage critically with its outputs. Rather than rejecting these tools outright, I believe the goal should be to develop frameworks for using them transparently and reflectively, especially in contexts such as education, where learning integrity is at stake.

Looking ahead, I see the role of computing professionals expanding. It is no longer enough to build systems that are efficient or reliable. We are also expected to think about how those systems affect people and communities. This aligns with the expectations set by professional bodies such as the British Computer Society (BCS), which calls for members to act in the public interest and avoid behaviour that could damage trust in the profession. It also echoes broader international efforts, such as UNESCO’s Recommendation on the Ethics of Artificial Intelligence, which promotes human-centred and inclusive AI development.

In conclusion, generative AI presents both great promise and significant challenges. The tools themselves are not unethical, but how we use them determines their impact. The work of Correa et al. (2023) and Deckard (2023) makes it clear that computing professionals must be prepared to think critically, act responsibly, and engage with emerging governance frameworks. Embedding ethical reflection into our everyday practices is essential if we want to guide AI in a direction that benefits society.

**References**

Correa, J., et al. (2023). Mapping AI Governance Across the Globe.

Deckard, J. (2023). Designing Ethics into AI Systems.

Finn, M. and Shilton, K. (2023) ‘Ethics governance development: The case of the Menlo Report’, Social Studies of Science, 53(3), pp. 315–340.

Fjeld, J., Achten, N., Hilligoss, H., Nagy, A. and Srikumar, M. (2020) Principled artificial intelligence: Mapping consensus in ethical and rights-based approaches to principles for AI. Berkman Klein Center Research Publication. Available at: [https://cyber.harvard.edu/publication/2020/principled-ai](https://cyber.harvard.edu/publication/2020/principled-ai)

Zhou, Y., Chen, H., Jiang, S., & Liu, Y. (2022). An Empirical Study on Code Generation with GitHub Copilot. arXiv preprint arXiv:2205.15398.

UNESCO (2021). Recommendation on the Ethics of Artificial Intelligence.

BCS (2018). Code of Conduct. British Computer Society.

